{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7558d7f-c52f-440f-a596-f30dca959b73",
   "metadata": {},
   "source": [
    "# Quantum k-means Clustering on local quantum simulator\n",
    "\n",
    "To run this program, please follow these steps:\n",
    "* Download the file.\n",
    "* Install Qiskit, following this instruction: https://docs.quantum.ibm.com/start/install#local\n",
    "* Install sklearn using this command: `pip install scikit-learn`\n",
    "* Restart Kernel and Run All Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca0f8e-b514-47a5-85cc-ff5a745b02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister\n",
    "from qiskit.primitives import StatevectorSampler\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\"\"\"\n",
    "Construct functions for changing Cartesian coordinate to Polar coordinate\n",
    "\"\"\"\n",
    "def polar_angle(point):\n",
    "    \"\"\"\n",
    "    Calculating polar angle\n",
    "    \"\"\"\n",
    "    polar_angle = math.atan2(point[1], point[0])\n",
    "    return polar_angle\n",
    "\n",
    "def polar_radius(point):\n",
    "    \"\"\"\n",
    "    Calculating polar coordinate\n",
    "    \"\"\"\n",
    "    polar_radius = math.sqrt(point[0] ** 2 + point[1] ** 2)\n",
    "    return polar_radius\n",
    "\n",
    "def initialize_clusters(k, dataset):\n",
    "    clusters = {}\n",
    "    # Randomly choose k points from the dataset as centroids\n",
    "    centroids = random.sample(dataset, k)\n",
    "    # Create a dictionary to store points in the cluster represented by centroids\n",
    "    for centroid in centroids:\n",
    "        clusters[f\"{centroid}\"] = []\n",
    "    return clusters, centroids\n",
    "\n",
    "def recompute_centroids(clusters, centroids):\n",
    "    \"\"\"\n",
    "    Construct function for recomputing centroids as the average of values in the clusters\n",
    "    \"\"\"\n",
    "    is_equal = True\n",
    "    new_clusters = {}\n",
    "    new_centroids = []\n",
    "    for old_centroids_string, cluster in clusters.items():\n",
    "        old_centroid = ast.literal_eval(old_centroids_string)\n",
    "        if len(cluster) > 0:\n",
    "            new_centroid = np.mean(cluster, axis=0).tolist()\n",
    "            if new_centroid != old_centroid:\n",
    "                is_equal = False\n",
    "            new_clusters[f\"{new_centroid}\"] = []\n",
    "            new_centroids.append(new_centroid)\n",
    "    if is_equal == False:\n",
    "        return new_clusters, new_centroids, is_equal\n",
    "    else:\n",
    "        return clusters, centroids, is_equal\n",
    "\n",
    "def quantum_circuit(qc, qr, cr, i, data_angle, centroid_angle):\n",
    "    qc.h(qr[i * 2])\n",
    "    qc.cx(qr[i * 2], qr[i * 2 + 1])\n",
    "    qc.ry(-abs(data_angle - centroid_angle), qr[i * 2 + 1])\n",
    "    qc.cx(qr[i * 2], qr[i * 2 + 1])\n",
    "    qc.ry(abs(data_angle - centroid_angle), qr[i * 2 + 1])\n",
    "    # Inteferernce and measurement\n",
    "    qc.h(qr[i * 2])\n",
    "    qc.measure(qr[i * 2], cr[i])\n",
    "    return qc\n",
    "\n",
    "def nearest_centroids_dictionary(dataset):\n",
    "    \"\"\"\n",
    "    Construct a dictionary to store the information on nearest centroid for each point in the dataset\n",
    "    \"\"\"\n",
    "    nearest_centroids_dict = {}\n",
    "    for data_point in dataset:\n",
    "        nearest_centroids_dict[f\"{data_point}\"] = {\n",
    "            'smallest distance': float('inf'),\n",
    "            'nearest centroid': None\n",
    "        }\n",
    "    return nearest_centroids_dict\n",
    "\n",
    "def khan_quantum_distance(calculated_distances, nearest_centroids_dict, num_shots=1024):\n",
    "    for idx, item in calculated_distances.items():\n",
    "        probability_1 = item['count 1'] / num_shots\n",
    "        cen = item['pair'][0]\n",
    "        data = item['pair'][1]\n",
    "        # Calculate the distance from probability of |1>\n",
    "        normalize_value = math.sqrt(data[0] ** 2 + data[1] ** 2 + cen[0] ** 2 + cen[1] ** 2)\n",
    "        quantum_dist = normalize_value * math.sqrt(2 * probability_1)\n",
    "        # Compare with classical distance\n",
    "        classical_dist = classical_distance(data, cen)\n",
    "        print(f\"Khan quantum distance - {quantum_dist} - Classical distance - {classical_dist}\")\n",
    "        # Update the nearest centroids dictionary\n",
    "        if quantum_dist < nearest_centroids_dict[f\"{data}\"]['smallest distance']:\n",
    "            nearest_centroids_dict[f\"{data}\"]['smallest distance'] = quantum_dist\n",
    "            nearest_centroids_dict[f\"{data}\"]['nearest centroid'] = cen\n",
    "    return nearest_centroids_dict\n",
    "\n",
    "def duong_quantum_distance(calculated_distances, nearest_centroids_dict, num_shots=1024):\n",
    "    for idx, item in calculated_distances.items():\n",
    "        probability_1 = item['count 1'] / num_shots\n",
    "        cen = item['pair'][0]\n",
    "        data = item['pair'][1]\n",
    "        # Calculate the distance from the probability of |1>\n",
    "        data_radius = polar_radius(data)\n",
    "        centroid_radius = polar_radius(cen)\n",
    "        quantum_dist = math.sqrt((data_radius - centroid_radius) ** 2 + 4 * data_radius * centroid_radius * probability_1)\n",
    "        # Compare with classical distance\n",
    "        classical_dist = classical_distance(data, cen)\n",
    "        print(f\"Duong quantum distance - {quantum_dist} - Classical distance - {classical_dist}\")\n",
    "        # Update the nearest centroids dictionary\n",
    "        if quantum_dist < nearest_centroids_dict[f\"{data}\"]['smallest distance']:\n",
    "            nearest_centroids_dict[f\"{data}\"]['smallest distance'] = quantum_dist\n",
    "            nearest_centroids_dict[f\"{data}\"]['nearest centroid'] = cen\n",
    "    return nearest_centroids_dict\n",
    "\n",
    "def classical_distance(point_1, point_2):\n",
    "    \"\"\"\n",
    "    Classically calculate the Euclidean distance between 2 points\n",
    "    \"\"\"\n",
    "    euclidean_distance = math.sqrt((point_1[0] - point_2[0]) ** 2 + (point_1[1] - point_2[1]) ** 2)\n",
    "    return euclidean_distance\n",
    "\n",
    "def classical_selection(dataset, centroids):\n",
    "    nearest_centroids_dict = nearest_centroids_dictionary(dataset)\n",
    "    for data in dataset:\n",
    "        for centroid in centroids:\n",
    "            classical_dist = classical_distance(data, centroid)\n",
    "            if classical_dist < nearest_centroids_dict[f\"{data}\"]['smallest distance']:\n",
    "                nearest_centroids_dict[f\"{data}\"]['smallest distance'] = classical_dist\n",
    "                nearest_centroids_dict[f\"{data}\"]['nearest centroid'] = centroid\n",
    "    return nearest_centroids_dict\n",
    "\n",
    "def khan_quantum_selection(dataset, centroids, num_qubits, num_shots=1024):\n",
    "    # Calculate the number of distances can be calculated at the same time\n",
    "    num_distances = 0\n",
    "    num_distances, remainder = divmod(num_qubits, 2)\n",
    "    # Initialize the dictionary contains information about nearest centroids for each data point\n",
    "    nearest_centroids_dict = nearest_centroids_dictionary(dataset)\n",
    "    # Initialize a list contains all pairs of points in the data point\n",
    "    pairs = []\n",
    "    for centroid in centroids:\n",
    "        for data in dataset:\n",
    "            pairs.append([centroid, data])\n",
    "    # Loop until all pairs' distances are calculated\n",
    "    while len(pairs) > 0:\n",
    "        # Initialize quantum circuit\n",
    "        qr = QuantumRegister(num_distances * 2, name='q')\n",
    "        cr = ClassicalRegister(num_distances, name='c')\n",
    "        qc = QuantumCircuit(qr, cr)\n",
    "        # Initialize dictionary contains all pairs of points whose distances are going to be calculated\n",
    "        calculated_distances = {}\n",
    "        i = 0\n",
    "        # Loop until the circuit is full\n",
    "        while (len(calculated_distances) < num_distances) and (len(pairs) > 0):\n",
    "            pair = pairs.pop()\n",
    "            centroid = pair[0]\n",
    "            data_point = pair[1]\n",
    "            centroid_angle = polar_angle(centroid)\n",
    "            data_angle = polar_angle(data_point)\n",
    "            qc = quantum_circuit(qc, qr, cr, i, data_angle, centroid_angle)\n",
    "            # Update the index and calculated_distances\n",
    "            calculated_distances[i] = {'pair': pair, 'count 1': 0}\n",
    "            i += 1\n",
    "        sampler = StatevectorSampler()\n",
    "        job = sampler.run([qc], shots=num_shots)\n",
    "        result = job.result()[0]\n",
    "        count = result.data.c.get_counts()\n",
    "        state_list = list(count.keys())\n",
    "        for state in state_list:\n",
    "            state_count = count.get(state, 0)\n",
    "            for idx, bit in enumerate(state):\n",
    "                if bit == '1':\n",
    "                    if (len(calculated_distances) - idx - 1) >= 0:\n",
    "                        calculated_distances[len(calculated_distances) - idx - 1]['count 1'] += state_count\n",
    "        nearest_centroids_dict = khan_quantum_distance(calculated_distances, nearest_centroids_dict, num_shots)\n",
    "    return nearest_centroids_dict\n",
    "\n",
    "def duong_quantum_selection(dataset, centroids, num_qubits, num_shots=1024):\n",
    "    # Calculate the number of distances can be calculated at the same time\n",
    "    num_distances = 0\n",
    "    num_distances, remainder = divmod(num_qubits, 2)\n",
    "    # Initialize the dictionary contains information about nearest centroids for each data point\n",
    "    nearest_centroids_dict = nearest_centroids_dictionary(dataset)\n",
    "    # Initialize a list contains all pairs of points in the data point\n",
    "    pairs = []\n",
    "    for centroid in centroids:\n",
    "        for data in dataset:\n",
    "            pairs.append([centroid, data])\n",
    "    # Loop until all pairs' distances are calculated\n",
    "    while len(pairs) > 0:\n",
    "        # Initialize quantum circuit\n",
    "        qr = QuantumRegister(num_distances * 2, name='q')\n",
    "        cr = ClassicalRegister(num_distances, name='c')\n",
    "        qc = QuantumCircuit(qr, cr)\n",
    "        # Initialize dictionary contains all pairs of points whose distances are going to be calculated\n",
    "        calculated_distances = {}\n",
    "        i = 0\n",
    "        # Loop until the circuit is full\n",
    "        while (len(calculated_distances) < num_distances) and (len(pairs) > 0):\n",
    "            pair = pairs.pop()\n",
    "            centroid = pair[0]\n",
    "            data_point = pair[1]\n",
    "            centroid_angle = polar_angle(centroid)\n",
    "            data_angle = polar_angle(data_point)\n",
    "            qc = quantum_circuit(qc, qr, cr, i, data_angle, centroid_angle)\n",
    "            # Update the index and calculated_distances\n",
    "            calculated_distances[i] = {'pair': pair, 'count 1': 0}\n",
    "            i += 1\n",
    "        sampler = StatevectorSampler()\n",
    "        job = sampler.run([qc], shots=num_shots)\n",
    "        result = job.result()[0]\n",
    "        count = result.data.c.get_counts()\n",
    "        state_list = list(count.keys())\n",
    "        for state in state_list:\n",
    "            state_count = count.get(state, 0)\n",
    "            for idx, bit in enumerate(state):\n",
    "                if bit == '1':\n",
    "                    if (len(calculated_distances) - idx - 1) >= 0:\n",
    "                        calculated_distances[len(calculated_distances) - idx - 1]['count 1'] += state_count\n",
    "        nearest_centroids_dict = duong_quantum_distance(calculated_distances, nearest_centroids_dict, num_shots)\n",
    "    return nearest_centroids_dict\n",
    "\n",
    "def khan_quantum_clustering(k, dataset, clusters, centroids, num_qubits, duong_iteration, num_shots=1024):\n",
    "    is_convergence = False\n",
    "    iteration_count = 0\n",
    "    repeated_point = [0.34600960858349594, -0.15629187416923862]\n",
    "    # Loop until convergence\n",
    "    while (not is_convergence) and (iteration_count < duong_iteration):\n",
    "        iteration_count += 1\n",
    "        # Run Khan's quantum program\n",
    "        khan_dict = khan_quantum_selection(dataset, centroids, num_qubits, num_shots)\n",
    "        classical_dict = classical_selection(dataset, centroids)\n",
    "        # Compare Khan's choice with classical choice\n",
    "        same_count = 0\n",
    "        different_count = 0\n",
    "        for data_string, data_dict in khan_dict.items():\n",
    "            # Count the number of times Khan and Classical methods chose the same (different) centroids\n",
    "            if data_dict['nearest centroid'] == classical_dict[data_string]['nearest centroid']:\n",
    "                same_count += 1\n",
    "            else:\n",
    "                different_count += 1\n",
    "            # Read and update the information to the clusters dictionary\n",
    "            data = ast.literal_eval(data_string)\n",
    "            nearest_centroid = data_dict['nearest centroid']\n",
    "            clusters[f\"{nearest_centroid}\"].append(data)\n",
    "        print(f\"Khan same count: {same_count} and different count: {different_count}\")\n",
    "        # Add the repeated point to the clusters dictionary\n",
    "        for centroid_string, cluster in clusters.items():\n",
    "            if repeated_point in cluster:\n",
    "                cluster.append(repeated_point)\n",
    "        # Check if enough clusters have been created\n",
    "        is_k_clusters = True\n",
    "        for centroid_string, cluster in clusters.items():\n",
    "            if len(cluster) == 0:\n",
    "                is_k_clusters = False\n",
    "        # If not enough clusters, repeat the upper task with another k randomly generated centroids\n",
    "        if is_k_clusters == False:\n",
    "            clusters, centroids = initalize_clusters(k, dataset)\n",
    "            continue\n",
    "        else:\n",
    "            # Recompute centroids by averaging all points\n",
    "            new_clusters, new_centroids, is_equal = recompute_centroids(clusters, centroids)\n",
    "            if is_equal == True:\n",
    "                is_convergence = True\n",
    "                break\n",
    "            else:\n",
    "                clusters = new_clusters\n",
    "                centroids = new_centroids\n",
    "    print(f\"Khan's iteration count is: {iteration_count}\")\n",
    "    return clusters, centroids\n",
    "\n",
    "def duong_quantum_clustering(k, dataset, clusters, centroids, num_qubits, num_shots=1024):\n",
    "    is_convergence = False\n",
    "    iteration_count = 0\n",
    "    repeated_point = [0.34600960858349594, -0.15629187416923862]\n",
    "    # Loop until convergence\n",
    "    while not is_convergence:\n",
    "        iteration_count += 1\n",
    "        # Run Duong's quantum program\n",
    "        duong_dict = duong_quantum_selection(dataset, centroids, num_qubits, num_shots)\n",
    "        classical_dict = classical_selection(dataset, centroids)\n",
    "        # Compare Duong's choice with classical choice\n",
    "        same_count = 0\n",
    "        different_count = 0\n",
    "        for data_string, data_dict in duong_dict.items():\n",
    "            # Count the number of times Duong and Classical methods chose the same (different) centroids\n",
    "            if data_dict['nearest centroid'] == classical_dict[data_string]['nearest centroid']:\n",
    "                same_count += 1\n",
    "            else:\n",
    "                different_count += 1\n",
    "            # Read and update the information to the clusters dictionary\n",
    "            data = ast.literal_eval(data_string)\n",
    "            nearest_centroid = data_dict['nearest centroid']\n",
    "            clusters[f\"{nearest_centroid}\"].append(data)\n",
    "        print(f\"Duong same count: {same_count} and different count: {different_count}\")\n",
    "        # Add the repeated point to the clusters dictionary\n",
    "        for centroid_string, cluster in clusters.items():\n",
    "            if repeated_point in cluster:\n",
    "                cluster.append(repeated_point)\n",
    "        # Check if enough clusters have been created\n",
    "        is_k_clusters = True\n",
    "        for centroid_string, cluster in clusters.items():\n",
    "            if len(cluster) == 0:\n",
    "                is_k_clusters = False\n",
    "        # If not enough clusters, repeat the upper task with another k randomly generated centroids\n",
    "        if is_k_clusters == False:\n",
    "            clusters, centroids = initalize_clusters(k, dataset)\n",
    "            continue\n",
    "        else:\n",
    "            # Recompute centroids by averaging all points\n",
    "            new_clusters, new_centroids, is_equal = recompute_centroids(clusters, centroids)\n",
    "            if is_equal == True:\n",
    "                is_convergence = True\n",
    "                break\n",
    "            else:\n",
    "                clusters = new_clusters\n",
    "                centroids = new_centroids\n",
    "    print(f\"Duong's iteration count is: {iteration_count}\")\n",
    "    return clusters, centroids, iteration_count\n",
    "\n",
    "def run_program(k, dataset, correct_clusters, num_qubits, num_shots=1024):\n",
    "    # Initialize empty clusters with randomly chosen centroids\n",
    "    clusters, centroids = initialize_clusters(k, dataset)\n",
    "    print(f\"Original centroids {centroids}\")\n",
    "    duong_clusters, duong_centroids, duong_iteration = duong_quantum_clustering(k, dataset, clusters, centroids, num_qubits, num_shots)\n",
    "    khan_clusters, khan_centroids = khan_quantum_clustering(k, dataset, clusters, centroids, num_qubits, duong_iteration, num_shots)\n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    # Khan plot\n",
    "    ax1 = axes[0]\n",
    "    # Plot original centroids\n",
    "    for centroid in centroids:\n",
    "        ax1.scatter(centroid[0], centroid[1], marker='o', s=100, color='black')\n",
    "    for centroid_string, cluster in khan_clusters.items():\n",
    "        # Plot centroid\n",
    "        centroid = ast.literal_eval(centroid_string)\n",
    "        ax1.scatter(centroid[0], centroid[1], marker='x', s=100, color='black')\n",
    "        # Plot points\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        for point in cluster:\n",
    "            x_values.append(point[0])\n",
    "            y_values.append(point[1])\n",
    "        ax1.scatter(x_values, y_values)\n",
    "    ax1.set_xlabel('Feature 1')\n",
    "    ax1.set_ylabel('Feature 2')\n",
    "    ax1.set_title('Khan et al. result')\n",
    "    # Duong plot\n",
    "    ax2 = axes[1]\n",
    "    # Plot original centroids\n",
    "    for centroid in centroids:\n",
    "        ax2.scatter(centroid[0], centroid[1], marker='o', s=100, color='black')\n",
    "    for centroid_string, cluster in duong_clusters.items():\n",
    "        # Plot centroids\n",
    "        centroid = ast.literal_eval(centroid_string)\n",
    "        ax2.scatter(centroid[0], centroid[1], marker='x', s=100, color='black')\n",
    "        # Plot points\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        for point in cluster:\n",
    "            x_values.append(point[0])\n",
    "            y_values.append(point[1])\n",
    "        ax2.scatter(x_values, y_values)\n",
    "    ax2.set_xlabel('Feature 1')\n",
    "    ax2.set_ylabel('Feature 2')\n",
    "    ax2.set_title('New method result')\n",
    "    # Correct labels plot\n",
    "    ax3 = axes[2]\n",
    "    # Plot original centroids\n",
    "    for centroid in centroids:\n",
    "        ax3.scatter(centroid[0], centroid[1], marker='o', s=100, color='black')\n",
    "    for centroid_string, cluster in correct_clusters.items():\n",
    "        # Plot centroids\n",
    "        centroid = ast.literal_eval(centroid_string)\n",
    "        ax3.scatter(centroid[0], centroid[1], marker='x', s=100, color='black')\n",
    "        # Plot points\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        for point in cluster:\n",
    "            x_values.append(point[0])\n",
    "            y_values.append(point[1])\n",
    "        ax3.scatter(x_values, y_values)\n",
    "    ax3.set_xlabel('Feature 1')\n",
    "    ax3.set_ylabel('Feature 2')\n",
    "    ax3.set_title('Correct labels')\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(\"k_means_compare1.png\")\n",
    "    return duong_clusters, khan_clusters, correct_clusters\n",
    "\n",
    "# Import Iris dataset\n",
    "iris_data = load_iris()\n",
    "features = iris_data.data\n",
    "labels = iris_data.target\n",
    "# Apply MinMaxScaler to map data onto (0, 1)\n",
    "features = MinMaxScaler().fit_transform(features)\n",
    "# Reduce the number of features\n",
    "features = PCA(n_components=2).fit_transform(features)\n",
    "# Change features to list and create list which store correct labels\n",
    "data_list = features.tolist()\n",
    "correct_list = labels.tolist()\n",
    "# Create a clusters dictionary from the information obtained above\n",
    "correct_clusters = {0: [], 1: [], 2: []}\n",
    "for i, label in enumerate(correct_list):\n",
    "    if label == 0:\n",
    "        correct_clusters[0].append(data_list[i])\n",
    "    elif label == 1:\n",
    "        correct_clusters[1].append(data_list[i])\n",
    "    else:\n",
    "        correct_clusters[2].append(data_list[i])\n",
    "# Change the dictionary keys to centroids\n",
    "new_correct_clusters = {}\n",
    "for i, cluster in correct_clusters.items():\n",
    "    centroid = np.mean(cluster, axis=0).tolist()\n",
    "    new_correct_clusters[f\"{centroid}\"] = cluster\n",
    "correct_clusters = new_correct_clusters\n",
    "# Run the experiment\n",
    "duong_clusters, khan_clusters, correct_clusters = run_program(3, data_list, correct_clusters, 10, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc93b9-ffc4-4c3e-a202-189cf6ccc7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
